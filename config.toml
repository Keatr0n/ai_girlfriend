[global]
whisper_model_path = "/path/to/your/whisper/model.bin"
# Only used when assistant doesn't have llm / piper models set
default_llm_model_path = "/path/to/your/llm/model.gguf"
default_piper_model_path = "/path/to/your/piper/model.onnx"
# Optional tool file path. The llm can use this to run any top level function found in the .py file.
# An example is in this repo
tool_path = "/path/to/your/tools.py"
# LLM Configuration
llm_threads = 16
llm_context_size = 128000
# if this is enabled, it will both show the words as they appear from the llm, and speak the audio in chunks
enable_word_by_word_response = true
# Optional: set a default assistant to skip the selection prompt
default_assistant = "Jarvis"
# If there is only one assistant present, it will be selected by default

[[assistant]]
# The assistant will refer to itself by this name
name = "Jarvis"
system_prompt = "You are an AI assistant. Keep your responses short and helpful. You respond with no formatting at all, only plain text."
# llm_model_path = "/path/to/model.gguf"  # Override the default from .env
# piper_model_path = "/path/to/model.onnx"  # Override the default from .env
# conversation_file = "Jarvis_history.txt"  # Defaults to {name}_history.txt
# tool_path = "/path/to/your/tools.py"
